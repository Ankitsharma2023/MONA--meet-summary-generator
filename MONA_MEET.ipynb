{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nbformat\n",
        "\n",
        "import nbformat\n",
        "\n",
        "# Read the current notebook\n",
        "with open('/content/MONA.ipynb', 'r') as f:\n",
        "    nb = nbformat.read(f, as_version=4)\n",
        "\n",
        "# Remove widget metadata if it exists\n",
        "if 'widgets' in nb.metadata:\n",
        "    del nb.metadata['widgets']\n",
        "    print(\"Removed 'widgets' metadata\")\n",
        "\n",
        "# Write the fixed version\n",
        "with open('/content/MONA_fixed.ipynb', 'w') as f:\n",
        "    nbformat.write(nb, f)\n",
        "\n",
        "print(\"Fixed notebook saved as MONA_fixed.ipynb\")"
      ],
      "metadata": {
        "id": "uIXMbY-rs6pq",
        "outputId": "87536032-283a-4cfc-a705-ed76f6042ee5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.12/dist-packages (5.10.4)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat) (2.21.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat) (4.25.1)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from nbformat) (5.9.1)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.12/dist-packages (from nbformat) (5.7.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (0.28.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat) (4.15.0)\n",
            "Removed 'widgets' metadata\n",
            "Fixed notebook saved as MONA_fixed.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lO1FU771ZB04"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70FZmeNJZJn3"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLiQVmPuZNay"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline,AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from IPython.display import  clear_output\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9k50KhQsZQAC"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "def convert_mp4_to_mp3(input_file, output_file):\n",
        "    \"\"\"Convert an MP4 file to MP3 using ffmpeg.\"\"\"\n",
        "    try:\n",
        "        command = [\n",
        "            \"ffmpeg\",\n",
        "            \"-i\", input_file,          # Input file\n",
        "            \"-vn\",                     # Disable video\n",
        "            \"-acodec\", \"libmp3lame\",   # Use MP3 codec\n",
        "            \"-b:a\", \"192k\",            # MP3 bitrate\n",
        "            output_file                # Output file\n",
        "        ]\n",
        "        subprocess.run(command, check=True)\n",
        "        print(f\"Conversion successful: {output_file}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(\"Error during conversion:\", e)\n",
        "\n",
        "# Run conversion on your newly uploaded file\n",
        "convert_mp4_to_mp3(\"Weekly Meeting Example .mp4\", \"meeting.mp3\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAHpm5zjZRw6"
      },
      "outputs": [],
      "source": [
        "pipe  = pipeline(\"automatic-speech-recognition\",\n",
        "                    \"openai/whisper-small\",\n",
        "                    chunk_length_s=30,\n",
        "                    stride_length_s=5,\n",
        "                    return_timestamps=True,\n",
        "                    device=device,\n",
        "                    generate_kwargs = {\"language\": 'english', \"task\": \"transcribe\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqlVkXvZZTeW"
      },
      "outputs": [],
      "source": [
        "transcription = pipe(\"meeting.mp3\" )\n",
        "#Once the transcription is complete, we’ll format the text with timestamps for better readability.\n",
        "formatted_lyrics = \"\"\n",
        "for line in transcription['chunks']:\n",
        "    text = line[\"text\"]\n",
        "    ts = line[\"timestamp\"]\n",
        "    formatted_lyrics += f\"{ts}-->{text}\\n\"\n",
        "\n",
        "print(formatted_lyrics.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ij6dm3yYZVy6"
      },
      "outputs": [],
      "source": [
        "# Let’s also save the transcription to a text file so we can use it later!\"\n",
        "with open(\"transcription.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(formatted_lyrics.strip())\n",
        "\n",
        "print(\"Transcription saved to transcription.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqWoOY_UZaUa"
      },
      "outputs": [],
      "source": [
        "DEFAULT_MODEL = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    DEFAULT_MODEL,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=device,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8n8SyS_AZcI2"
      },
      "outputs": [],
      "source": [
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"You are an expert meeting summarizer. Your job is to read the user's full meeting \"\n",
        "            \"transcript and produce a clear, structured, accurate summary.\\n\\n\"\n",
        "            \"The summary should include:\\n\"\n",
        "            \"- Main topics discussed\\n\"\n",
        "            \"- Key decisions made\\n\"\n",
        "            \"- Action items (with who will do what)\\n\"\n",
        "            \"- Important concerns or issues raised\\n\"\n",
        "            \"- Any follow-ups required\\n\\n\"\n",
        "            \"DO NOT add any extra topics. ONLY use information from the transcript.\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": formatted_lyrics.strip()\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "# print(prompt)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=True,\n",
        "        max_new_tokens=1000\n",
        "    )\n",
        "\n",
        "processed_text = tokenizer.decode(output[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
        "\n",
        "print(processed_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MU7neWVmZdIS"
      },
      "outputs": [],
      "source": [
        "print(formatted_lyrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXPHdnngZ1c2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}